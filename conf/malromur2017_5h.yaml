asr:
  optimizer:
    type: 'Adadelta'                                        # Optimizer used for training (not mentioned in LAS)      
    learning_rate: 1.0                                      # Learning rate for opt (not mentioned in LAS)
  
  model_para:
    encoder_state_size: 256                                 # The state size of the LSTM used in the encoder (LAS approved)
    mlp_out_size: 128                                       # The dimension projected to by the phi and psi nets
    decoder_state_size: 256                                 # The state size of the LSTM used in the decoder (LAS approved)
    tf_rate: 0.9                                            # Teacher forcing rate (LAS approved)
    feature_dim: 40                                         # Dimensionality of fbanks (has to be same as used in prepro) (LAS approved)
  
  train_index: './processed_data/malromur2017/production_indexes/5hour_byxlen.tsv'
  valid_index: './processed_data/malromur2017/production_indexes/eval_index_byxlen.tsv'
  test_index: './processed_data/malromur2017/production_indexes/100_by_xlen.tsv'

  decode_beam_size: 3
  decode_jobs: 8
  max_decode_step_ratio: 0.25
  decode_lm_weight: 0.5

  wer_step: 50

  train_batch_size: 32
  valid_batch_size: 32
  test_batch_size: 1
  
  n_epochs: 100
  valid_step: 1000
  save_step: 500
  logging_step: 1
  sanity_steps: 200

sae:
  optimizer:
    type: 'Adam'                                        # Optimizer used for training speech auto encoder (Drexler approved)
    learning_rate: 0.0001                                      # Learning rate for opt (Drexler approved)
  model_para:
    kernel_sizes: [[1, 36], [5, 1], [3, 1]]                 # A list of kernel sizes for the 3 convolutional layers
    num_filters: [32, 64, 256]                              # A list of output filters for the 3 convolutional layers
    pool_kernel_sizes: [[3, 1], [5, 1], [2000, 40]]         # A list of kernel sizes (only 3 values) for the max pooling of each convolutional layer
  
  train_index: './processed_data/malromur2017/production_indexes/train_index_byxlen.tsv'
  valid_index: './processed_data/malromur2017/production_indexes/eval_index_byxlen.tsv'
  
  train_batch_size: 32
  eval_batch_size: 32
  n_epochs: 10
  valid_step: 1000
  save_step: 200
  logging_step: 1
  sanity_steps: 200
  
tae:
  optimizer:
    type: 'Adam'                                        # Optimizer used for training speech auto encoder (Drexler approved)
    learning_rate: 0.0001                                      # Learning rate for opt (Drexler approved)
  model_para:
    state_size: 256
    emb_dim: 128
    num_layers: 2
  drop_rate: 0.1                                            # The probability of a character being dropped in the input
  
  train_index: './processed_data/malromur2017/production_indexes/train_index_byxlen.tsv'
  valid_index: './processed_data/malromur2017/production_indexes/eval_index_byxlen.tsv'
  
  train_batch_size: 64
  eval_batch_size: 64
  n_epochs: 50
  save_step: 2000
  valid_step: 1000
  logging_step: 1

adv:
  G_optimizer:
    type: 'Adadelta'
    learning_rate: 1.0
  D_optimizer:
    type: 'Adadelta'
    learning_rate: 1.0  
  model_para:
    hidden_dim: 256
  label_smoothing: 0.1

  train_index: './processed_data/malromur2017/production_indexes/train_index_byxlen.tsv'
  eval_index: './processed_data/malromur2017/production_indexes/eval_index_byxlen.tsv'

  train_batch_size: 32
  eval_batch_size: 32

  n_epochs: 10
  save_step: 1000
  valid_step: 1000
  logging_step: 1

char_lm:
  optimizer:
    type: 'Adam'                                      # Optimizer used for training (not mentioned in LAS)      
    learning_rate: 0.0001                                 # Learning rate for opt (not mentioned in LAS)
  model_para:
    hidden_size: 128
    tf_rate: 0.9                                            # Teacher forcing rate (LAS approved)

  train_index: './processed_data/risamalromur/train.txt'
  valid_index: './processed_data/risamalromur/eval.txt'

  chunk_size: 200

  train_batch_size: 128
  n_epochs: 8
  valid_step: 100
  logging_step: 1
  save_step: 50

seed_gen:
  super_its: 5