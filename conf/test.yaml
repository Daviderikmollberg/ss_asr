asr_model:
  optimizer:
    type: 'Adadelta'                               # Optimizer used for training (not mentioned in LAS)      
    learning_rate: 1.0                             # Learning rate for opt (not mentioned in LAS)
  encoder:                                      
    state_size: 256                                # The state size of the LSTM used in the encoder (LAS approved)
  attention:                                             
    mlp_out_size: 128                              # The dimension projected to by the phi and psi nets
  decoder:                                             
    state_size: 256                                # The state size of the LSTM used in the decoder (LAS approved)
  tf_rate: 0.9                                     # Teacher forcing rate (LAS approved)
  feature_dim: 40                                  # Dimensionality of fbanks (has to be same as used in prepro) (LAS approved)

speech_autoencoder:
  optimizer:
    type: 'Adadelta'                               # Optimizer used for training speech auto encoder
    learning_rate: 1.0                             # Learning rate for opt
  kernel_sizes: [[36, 1], [1, 5], [1, 3]]          # A list of kernel sizes for the 3 convolutional layers
  num_filters: [32, 64, 256]                       # A list of output filters for the 3 convolutional layers
  pool_kernel_sizes: [[3, 1], [5, 1], [2000, 40]]  # A list of kernel sizes (only 3 values) for the max pooling of each convolutional layer
    
text_autoencoder:
  optimizer:
    type: 'Adadelta'
    learning_rate: 1.0
  state_size: 256
  emb_dim: 128
  num_layers: 2
  drop_rate: 0.1                                   # The probability of a character being dropped in the input

discriminator:
  G_optimizer:
    type: 'Adadelta'
    learning_rate: 1.0
  D_optimizer:
    type: 'Adadelta'
    learning_rate: 1.0  
  hidden_dim: 256
  label_smoothing: 0.1

rnn_lm:
  optimizer: 
    type: 'Adam'                                   # Optimizer used for training
    learning_rate: 0.001                           # Learning rate for optimizer
  model_para:           
    emb_dim: 650                                   # Word embedding dimensionality
    hidden_dim: 650                                # Hidden dimensionality of RNN unit
    num_layers: 2                                  # Number of RNN layers
    dropout_rate: 0.5                              # Dropout rate used in RNNLM

solver:
  # Paths to prepro indexes
  train_index_path_byxlen: './data/processed/index_byxlen.tsv'
  train_index_path_byylen: './data/processed/index_byylen.tsv'
  eval_index_path_byxlen: './data/processed/eval_index_byxlen.tsv'
  eval_index_path_byylen: './data/processed/eval_index_byylen.tsv'
  test_index_path: None
  n_jobs: 8                                        # Number of subprocesses used for Dataloader (LAS neutral)
  batch_size: 2                                    # training batch size (LAS approved)
  total_steps: 100                                 # total steps for training (LAS neutral)
  eval_batch_size: 2                               # validation batch size (LAS approved)                                                         
  eval_step: 2                                     # interval of validation steps (not mentioned in LAS)
  # Path to pretrained LM
  decode_lm_path: 'result/complete_ivona_lm/rnnlm'
  decode_lm_weight: 0.5

