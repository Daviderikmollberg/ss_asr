asr_model:
  optimizer:
    type: 'Adadelta'                               # Optimizer used for training (not mentioned in LAS)      
    learning_rate: 1.0                             # Learning rate for opt (not mentioned in LAS)
  model_para:
    encoder_state_size: 256                                # The state size of the LSTM used in the encoder (LAS approved)
    mlp_out_size: 128                              # The dimension projected to by the phi and psi nets
    decoder_state_size: 256                                # The state size of the LSTM used in the decoder (LAS approved)
    tf_rate: 0.9                                     # Teacher forcing rate (LAS approved)
    feature_dim: 40                                  # Dimensionality of fbanks (has to be same as used in prepro) (LAS approved)
  decode_lm_weight: 0.5 

speech_autoencoder:
  optimizer:
    type: 'Adadelta'                                    # Optimizer used for training speech auto encoder (Drexler approved)
    learning_rate: 1.0                             # Learning rate for opt (Drexler approved)
  model_para:
    kernel_sizes: [[36, 1], [1, 5], [1, 3]]          # A list of kernel sizes for the 3 convolutional layers
    num_filters: [32, 64, 256]                       # A list of output filters for the 3 convolutional layers
    pool_kernel_sizes: [[3, 1], [5, 1], [2000, 40]]  # A list of kernel sizes (only 3 values) for the max pooling of each convolutional layer
    
text_autoencoder:
  optimizer:
    type: 'Adadelta'
    learning_rate: 1.0
  model_para:
    state_size: 256
    emb_dim: 128
    num_layers: 2
  drop_rate: 0.1                                   # The probability of a character being dropped in the input

discriminator:
  G_optimizer:
    type: 'Adadelta'
    learning_rate: 1.0
  D_optimizer:
    type: 'Adadelta'
    learning_rate: 1.0  
  model_para:
    hidden_dim: 256
  label_smoothing: 0.1

rnn_lm:
  optimizer: 
    type: 'Adam'                                   # Optimizer used for training
    learning_rate: 0.001                           # Learning rate for optimizer
  model_para:           
    emb_dim: 650                                   # Word embedding dimensionality
    hidden_dim: 650                                # Hidden dimensionality of RNN unit
    num_layers: 2                                  # Number of RNN layers
    dropout_rate: 0.5                              # Dropout rate used in RNNLM
  
solver:
  # Paths to prepro indexes
  train_index_path_byxlen: './processed_data/malromur2017/production_indexes/2_5hour_byxlen.tsv'
  train_index_path_byylen: './processed_data/malromur2017/production_indexes/train_index_byylen.tsv'
  eval_index_path_byxlen: './processed_data/malromur2017/production_indexes/eval_index_byxlen.tsv'
  eval_index_path_byylen: './processed_data/malromur2017/production_indexes/eval_index_byylen.tsv'
  test_index_path_byxlen: './processed_data/malromur2017/production_indexes/eval_index_byxlen.tsv'
  test_index_path_byylen: './processed_data/malromur2017/production_indexes/eval_index_byylen.tsv'

  n_jobs: 8                                        # Number of subprocesses used for Dataloader (LAS neutral)

  decode_beam_size: 20
  decode_jobs: 1
  max_decode_step_ratio: 0.25
  decode_lm_weight: 0.5

  batch_size: 32                                   # training batch size (LAS approved)
  eval_batch_size: 32
  test_batch_size: 1

  total_steps: 3000000                              # total steps for training (LAS neutral)
  eval_step: 3000                                  # interval of validation steps (not mentioned in LAS)
