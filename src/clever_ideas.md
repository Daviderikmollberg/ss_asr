* Try overfitting to a single batch on all modules
    [] LM
    [

* Try training on all-zero data to get underlying distribution by
inspecting the loss
* Use variance-scaled initialization

More advice: https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/
https://twitter.com/karpathy/status/1013244313327681536
