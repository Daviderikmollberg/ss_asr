import torch
import torch.nn as nn

class Discriminator(nn.Module):
    def __init__(self, in_dim:int, hidden_dim:int=256):
        '''
        Input arguments:
        in_dim (int): The feature dimension of samples supplied as inputs
        to the discriminator
        hidden_dim (int): Number of nodes per layer in the discriminator

        In our model, the discriminator is a simple feed-forward
        network with two fully-connected layers. It takes as input
        a single vector, and outputs a single real-valued score in the
        interval [0, 1]. 
        The discriminator is trained using binary cross-
        entropy loss to assign high scores to output vectors generated
        by the text encoder and low scores to those generated by the
        speech encoder. We use label smoothing
        '''
        super(Discriminator, self).__init__()

        self.in_layer = nn.Linear(in_dim, hidden_dim)
        self.l_1 = nn.Linear(hidden_dim, hidden_dim)
        self.l_2 = nn.Linear(hidden_dim, 1)
    
    def forward(self, x):
        out = self.in_layer(x)
        out = self.l_1(out)
        out = self.l_2(out)
        return out

def test_solver():
    '''
    Adversarial training:
    * (G)enerator: The speech encoder
    * (D)iscriminator: This discriminator module
    * Data distribution: The speech encoder

    D is trained to maximize the correct label to samples from
    either G (label = 0 ) or the data distribution (label = 1)
    --> maximize: log(D(x)) + log(1 - D(G(x)))

    G is trained to maximize the error rate of the D.
    --> maximize: log(D(G(z)))

    We use cross entropy loss for both and different optims for each
    '''
    from dataset import load_dataset, prepare_y
    from asr import Speller, Attention

    (mapper, dataset, noisy_dataloader) = load_dataset('data/processed/index.tsv', 
        batch_size=2, n_jobs=1, use_gpu=False, text_only=True, drop_rate=0.2)

    loss_metric = torch.nn.CrossEntropyLoss(ignore_index=0, 
            reduction='none').to(torch.device('cpu'))

    speller = Speller(256, 256*2)
    attention = Attention(128, 256*2, 256)
    char_emb = nn.Embedding(dataset.get_char_dim(), 256)
    char_trans = nn.Linear(256, dataset.get_char_dim())

    text_autoenc = TextAutoEncoder(dataset.get_char_dim(), 
        speller=speller, attention=attention, char_emb=char_emb, char_trans=char_trans,
        tf_rate=0.9)

    optim = torch.optim.SGD(text_autoenc.parameters(), lr=0.01, momentum=0.9)

    for (y, y_noise) in noisy_dataloader:
        print(y.shape)

        y, y_lens = prepare_y(y)
        y_max_len = max(y_lens)
        
        print(y.shape)

        y_noise, y_noise_lens = prepare_y(y_noise)
        y_noise_max_lens = max(y_noise_lens)
        
        print("Clean string (length={}) :{}".format(y_lens[0] , dataset.decode(y[0, :].view(-1))))
        print("Noisy string (length={}) :{}".format(y_noise_lens[0], 
            dataset.decode(y_noise[0, :].view(-1))))
        
        print("Clean lengths: {}".format(y_lens))
        print("Noise lengths: {}".format(y_noise_lens))


        optim.zero_grad()

        # decode steps == longest target
        decode_step = y_max_len 
        noise_lens, enc_out = text_autoenc(y, y_noise, decode_step, 
            noise_lens=y_noise_lens)

        print(enc_out.shape)
        print(y.shape)
        
        b,t,c = enc_out.shape
        loss = loss_metric(enc_out.view(b*t,c), y.view(-1))
        # Sum each uttr and devide by length
        loss = torch.sum(loss.view(b,t),dim=-1)/torch.sum(y!=0,dim=-1)\
            .to(device = torch.device('cpu'), dtype=torch.float32)
        # Mean by batch
        loss = torch.mean(loss)


        print("LOSSS :::::: {}".format(loss))

        loss.backward()
        optim.step()



if __name__ == '__main__': 
    d = Discriminator(50)

    sample = torch.randn(20, 50)

    print(d(sample))
